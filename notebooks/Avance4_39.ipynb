{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W94YPusvub-n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb93fbbc-bdbf-4276-e9c7-b8c7a5128381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.274\n",
            "  Downloading langchain-0.0.274-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.274)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.274)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (2.10.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.274) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.274) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.274) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.274) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.274) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.274) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.274) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.274) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274) (24.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.274)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.274 langsmith-0.0.92 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting gpt4all==1.0.8\n",
            "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all==1.0.8) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all==1.0.8) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==1.0.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==1.0.8) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==1.0.8) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all==1.0.8) (2024.2.2)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-1.0.8\n",
            "Collecting chromadb==0.4.7\n",
            "  Downloading chromadb-0.4.7-py3-none-any.whl (415 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.5/415.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (2.31.0)\n",
            "Collecting pydantic<2.0,>=1.9 (from chromadb==0.4.7)\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.2 (from chromadb==0.4.7)\n",
            "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.7)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.4.7)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.7)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (4.11.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.7)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.7)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.4.7)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (4.66.4)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.7)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.7) (6.4.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.7)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.7)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.7)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7) (24.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.7) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.7) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.7)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.7)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.7) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.7) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.7) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.7) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.7) (0.23.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.7) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.7) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.7)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.7) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.7) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.7)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.7) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.7) (1.2.1)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2318850 sha256=4ccf9879b154e9c495f16e9a6fa0a7c2dc5c3c15aa07f0ae4348495238d00b65\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=03802e04de312b8561a53899aa135a857487cf91826991879c2862b31d5738a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pydantic, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.1.3 chroma-hnswlib-0.7.2 chromadb-0.4.7 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.18.0 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.5.0 pydantic-1.10.15 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.27.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp310-cp310-linux_x86_64.whl size=3650241 sha256=91c1b56ac6bcdbd16846ee0242e82df010ad6fd5fcd147566939bdd27beb3e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e5/04/a5fa9e60033548f205f0db5f6ab6f59cd27bd0da7f9c51cfe7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.76\n",
            "Collecting urllib3==2.0.4\n",
            "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed urllib3-2.0.4\n",
            "Collecting PyMuPDF==1.23.1\n",
            "  Downloading PyMuPDF-1.23.1-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.0 (from PyMuPDF==1.23.1)\n",
            "  Downloading PyMuPDFb-1.23.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.1 PyMuPDFb-1.23.0\n",
            "Collecting python-dotenv==1.0.0\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.0.1\n",
            "    Uninstalling python-dotenv-1.0.1:\n",
            "      Successfully uninstalled python-dotenv-1.0.1\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting unstructured==0.10.8\n",
            "  Downloading unstructured-0.10.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.10.8)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured==0.10.8)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.10.8) (4.12.3)\n",
            "Collecting emoji (from unstructured==0.10.8)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.10.8) (2.5)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji->unstructured==0.10.8) (4.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.10.8) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.10.8) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.10.8) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.10.8) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.10.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.10.8) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.10.8) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.10.8) (2024.2.2)\n",
            "Installing collected packages: filetype, python-magic, emoji, unstructured\n",
            "Successfully installed emoji-2.12.1 filetype-1.2.0 python-magic-0.4.27 unstructured-0.10.8\n",
            "Collecting extract-msg==0.45.0\n",
            "  Downloading extract_msg-0.45.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imapclient<3,>=2.3.0 (from extract-msg==0.45.0)\n",
            "  Downloading IMAPClient-2.3.1-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile==0.46 (from extract-msg==0.45.0)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal<6,>=4.2 in /usr/local/lib/python3.10/dist-packages (from extract-msg==0.45.0) (5.2)\n",
            "Collecting compressed-rtf<2,>=1.0.6 (from extract-msg==0.45.0)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic<2,>=1.1.1 (from extract-msg==0.45.0)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<4.13,>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg==0.45.0) (4.12.3)\n",
            "Collecting RTFDE<0.2,>=0.1.0 (from extract-msg==0.45.0)\n",
            "  Downloading RTFDE-0.1.1-py3-none-any.whl (36 kB)\n",
            "Collecting red-black-tree-mod==1.20 (from extract-msg==0.45.0)\n",
            "  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<4.13,>=4.11.1->extract-msg==0.45.0) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imapclient<3,>=2.3.0->extract-msg==0.45.0) (1.16.0)\n",
            "Collecting lark==1.1.8 (from RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading lark-1.1.8-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oletools>=0.56 (from RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.2/977.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing<3,>=2.1.0 (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easygui (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorclass (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\n",
            "Collecting msoffcrypto-tool (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0)\n",
            "  Downloading msoffcrypto_tool-5.4.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=35.0 in /usr/local/lib/python3.10/dist-packages (from msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg==0.45.0) (2.22)\n",
            "Building wheels for collected packages: olefile, red-black-tree-mod, compressed-rtf\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=f68055483b0d941d547c60b2a0d3340e1a0809762b95af7e94c16f0a67ae7971\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "  Building wheel for red-black-tree-mod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18621 sha256=dca4a090c739d483f9e5be86fe0f751ea36bce2c40d223d49de28b72298a011f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/89/a0/17d08e78a59e4e8f51a95fe52e19c6916450c143acc7bce4dd\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=5fd322e3b8a80b6848ce4d14055273a60ef51535273023b687360401cae894f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "Successfully built olefile red-black-tree-mod compressed-rtf\n",
            "Installing collected packages: red-black-tree-mod, ebcdic, easygui, compressed-rtf, pyparsing, olefile, lark, imapclient, colorclass, msoffcrypto-tool, pcodedmp, oletools, RTFDE, extract-msg\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.2\n",
            "    Uninstalling pyparsing-3.1.2:\n",
            "      Successfully uninstalled pyparsing-3.1.2\n",
            "Successfully installed RTFDE-0.1.1 colorclass-2.2.2 compressed-rtf-1.0.6 easygui-0.98.3 ebcdic-1.1.1 extract-msg-0.45.0 imapclient-2.3.1 lark-1.1.8 msoffcrypto-tool-5.4.1 olefile-0.46 oletools-0.60.1 pcodedmp-1.2.6 pyparsing-2.4.7 red-black-tree-mod-1.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              },
              "id": "5012eb356e8143ce95705e5f658d6db1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Collecting tqdm==4.66.1\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "Successfully installed tqdm-4.66.1\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.7.0\n",
            "Collecting jq\n",
            "  Downloading jq-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.6/657.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.7.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.5.14)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (1.10.15)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, langsmith, jsonpatch, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.92\n",
            "    Uninstalling langsmith-0.0.92:\n",
            "      Successfully uninstalled langsmith-0.0.92\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.274\n",
            "    Uninstalling langchain-0.0.274:\n",
            "      Successfully uninstalled langchain-0.0.274\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langchain_community-0.2.1 langsmith-0.1.63 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.274\n",
        "!pip install gpt4all==1.0.8\n",
        "!pip install chromadb==0.4.7\n",
        "!pip install llama-cpp-python\n",
        "!pip install urllib3==2.0.4\n",
        "!pip install PyMuPDF==1.23.1\n",
        "!pip install python-dotenv==1.0.0\n",
        "!pip install unstructured==0.10.8\n",
        "!pip install extract-msg==0.45.0\n",
        "!pip install tabulate==0.9.0\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install sentence_transformers\n",
        "!pip install jq\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Third-Party Library Imports\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Langchain Imports\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    EverNoteLoader,\n",
        "    PyMuPDFLoader,\n",
        "    TextLoader,\n",
        "    JSONLoader\n",
        ")\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "from transformers import AutoModel, AutoTokenizer,  BertModel, BertTokenizer\n",
        "\n",
        "# ChromaDB Imports\n",
        "from chromadb.config import Settings\n",
        "import chromadb\n",
        "\n",
        "# Argument Parsing\n",
        "import argparse\n",
        "\n",
        "import torch  # Import PyTorch to check GPU availability\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "8vR6uFvsv0I0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"./db/\"\n",
        "#model_type = \"GPT4All_GROOVY\"\n",
        "#model_type = \"GPT4All_SNOOZY\"\n",
        "#model_type = \"VLLM\"\n",
        "model_type = \"LLaMA_2_7B\"\n",
        "source_directory = \"/content/drive/MyDrive/Colab/SOR/\"\n",
        "embeddings_model_name = \"all-MiniLM-L6-v2\"\n",
        "model_n_ctx = 1000\n",
        "model_n_batch = 8\n",
        "target_source_chunks = 4\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50"
      ],
      "metadata": {
        "id": "-lVGkrCowkZW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LXhY9iysa-ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0b0409-e20d-49e8-e331-042ecf47a1d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the specified source directory\n",
        "archivos = os.listdir(source_directory)\n",
        "\n",
        "# Count the number of files in the directory\n",
        "cantidad_de_archivos = len(archivos)\n",
        "\n",
        "# Print the number of files in the directory\n",
        "print(f\"The folder '{source_directory}' contains {cantidad_de_archivos} files.\")\n"
      ],
      "metadata": {
        "id": "jwRoOfK7brJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757fa90c-1922-4f57-8bed-b32cd646089e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The folder '/content/drive/MyDrive/Colab/SOR/' contains 2 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "nPbkWegG7X8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a3dcc7-0868-44ab-aba9-89913b759c14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings using Hugging Face model\n",
        "# The 'embeddings_model_name' specifies the pre-trained model to use for embeddings.\n",
        "\n",
        "# Load the model and tokenizer from transformers, specifying the device\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n"
      ],
      "metadata": {
        "id": "9GbEaG35wm34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f895a2e-f297-4abe-9c63-2717e0a76941"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map file extensions to document loaders and their arguments\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {})  # Use the CSVLoader for .csv files with no additional arguments.\n",
        "    # Add more mappings for other file extensions and loaders as needed\n",
        "}\n"
      ],
      "metadata": {
        "id": "RmhR8Aebyeo_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a single document based on its file path\n",
        "def load_single_document(file_path: str) -> List[Document]:\n",
        "    # Extract the file extension from the given file path.\n",
        "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
        "\n",
        "    # Check if the file extension is in the LOADER_MAPPING dictionary.\n",
        "    if ext in LOADER_MAPPING:\n",
        "        # Get the loader class and loader arguments for the specified extension.\n",
        "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
        "\n",
        "        # Create an instance of the loader class with the specified file path and arguments.\n",
        "        loader = loader_class(file_path, **loader_args)\n",
        "\n",
        "        # Load the document using the loader and return it.\n",
        "        return loader.load()\n",
        "\n",
        "    # If the file extension is not supported, raise a ValueError.\n",
        "    raise ValueError(f\"Unsupported file extension '{ext}'\")"
      ],
      "metadata": {
        "id": "otUBD37exX5J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from the source directory\n",
        "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
        "    # Find all files in the source directory with extensions specified in LOADER_MAPPING.\n",
        "    all_files = []\n",
        "    for ext in LOADER_MAPPING:\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
        "        )\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
        "        )\n",
        "\n",
        "    # Filter out files that are in the ignored_files list.\n",
        "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
        "\n",
        "    # Use a multiprocessing Pool to load documents in parallel.\n",
        "    with Pool(processes=os.cpu_count()) as pool:\n",
        "        results = []\n",
        "        # Create a progress bar for loading documents.\n",
        "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
        "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
        "                results.extend(docs)\n",
        "                pbar.update()\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "Qd4JTeSExbXS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process documents into text chunks\n",
        "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
        "    # Print a message indicating that documents are being loaded from the specified source directory.\n",
        "    print(f\"Loading documents from {source_directory}\")\n",
        "\n",
        "    # Load documents from the source directory, excluding any ignored files.\n",
        "    documents = load_documents(source_directory, ignored_files)\n",
        "\n",
        "    # Check if there are no documents to process and exit if that's the case.\n",
        "    if not documents:\n",
        "        print(\"No new documents to load\")\n",
        "        exit(0)\n",
        "\n",
        "    # Print the number of loaded documents and the source directory.\n",
        "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
        "\n",
        "    # Create a text splitter with the specified chunk size and overlap.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    # Split the loaded documents into chunks of text using the text splitter.\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Print the number of text chunks created and the maximum chunk size.\n",
        "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
        "\n",
        "    # Return the resulting text chunks.\n",
        "    return texts"
      ],
      "metadata": {
        "id": "kfG4Hux3xegk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if the vector store already exists\n",
        "def does_vectorstore_exist(persist_directory: str, embeddings: HuggingFaceEmbeddings) -> bool:\n",
        "    # Create a Chroma vector store instance with the specified persist directory and embeddings.\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    # Get the 'documents' data from the vector store. If it's empty, return False; otherwise, return True.\n",
        "    if not db.get()['documents']:\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "WIborcoMxhEC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the vector store already exists in the specified directory with the given embeddings.\n",
        "if does_vectorstore_exist(persist_directory, embeddings):\n",
        "    # If the vector store exists, append to it.\n",
        "    print(f\"Appending to existing vector store at {persist_directory}\")\n",
        "\n",
        "    # Create a Chroma vector store instance with the specified directory and embeddings.\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    # Get the existing collection from the vector store.\n",
        "    collection = db.get()\n",
        "\n",
        "    # Extract source file paths from the collection's metadata.\n",
        "    source_file_paths = [metadata['source'] for metadata in collection['metadatas']]\n",
        "\n",
        "    # Process the documents based on the extracted source file paths.\n",
        "    texts = process_documents(source_file_paths)\n",
        "\n",
        "    # Inform the user about the embeddings creation process.\n",
        "    print(f\"Creating embeddings. May take some minutes...\")\n",
        "\n",
        "       # Check if 'texts' is not empty before adding documents to the vector store\n",
        "    if texts:\n",
        "        # Add the processed documents to the vector store.\n",
        "        db.add_documents(texts)\n",
        "    else:\n",
        "        print(\"No documents to add. Skipping insertion.\")\n",
        "else:\n",
        "    # If the vector store does not exist, create a new one.\n",
        "    print(\"Creating a new vector store\")\n",
        "\n",
        "    # Process documents without specifying ignored files (default behavior).\n",
        "    texts = process_documents()\n",
        "\n",
        "    # Inform the user about the embeddings creation process.\n",
        "    print(f\"Creating embeddings. May take some minutes...\")\n",
        "\n",
        "    # Create a new Chroma vector store with the processed documents and embeddings.\n",
        "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "# Persist the vector store.\n",
        "db.persist()\n",
        "\n",
        "# Clear the db variable to free up resources.\n",
        "db = None\n",
        "\n",
        "# Inform the user that the ingestion process is complete.\n",
        "print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")\n"
      ],
      "metadata": {
        "id": "HI66G_HcxjSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2cc880-1858-41d7-8474-feffd070aaf8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to existing vector store at ./db/\n",
            "Loading documents from /content/drive/MyDrive/Colab/SOR/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading new documents: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new documents to load\n",
            "Loaded 0 new documents from /content/drive/MyDrive/Colab/SOR/\n",
            "Split into 0 chunks of text (max. 500 tokens each)\n",
            "Creating embeddings. May take some minutes...\n",
            "No documents to add. Skipping insertion.\n",
            "Ingestion complete! You can now run privateGPT.py to query your documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create settings for Chroma database configuration\n",
        "settings = Settings(\n",
        "    persist_directory=persist_directory,  # Directory for persisting database data\n",
        "    anonymized_telemetry=False  # Disable anonymized telemetry\n",
        ")\n",
        "\n",
        "# Create Hugging Face embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "# Create a persistent Chroma database client\n",
        "chroma_client = chromadb.PersistentClient(\n",
        "    settings=settings,  # Database settings\n",
        "    path=persist_directory  # Path to the database directory\n",
        ")\n",
        "\n",
        "# Create a Chroma vector store instance\n",
        "db = Chroma(\n",
        "    persist_directory=persist_directory,  # Directory for persisting vector store data\n",
        "    embedding_function=embeddings,  # Embeddings function\n",
        "    client_settings=settings,  # Database settings\n",
        "    client=chroma_client  # Chroma client\n",
        ")\n",
        "\n",
        "# Create a retriever for document retrieval\n",
        "retriever = db.as_retriever(\n",
        "    search_kwargs={\"k\": target_source_chunks}  # Search settings (e.g., number of search results)\n",
        ")\n"
      ],
      "metadata": {
        "id": "pk7vv2QczkIx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación de Modelos de Lenguaje Natural\n",
        "\n",
        "1. LLaMA 2 (LlamaCpp)\n",
        "\n",
        "Descripción: LLaMA 2 es una variante del modelo LLaMA, optimizada para funcionar de manera eficiente en distintas plataformas. Utiliza el backend llama-cpp para realizar inferencias de manera rápida.\n",
        "Características Técnicas:\n",
        "Ruta del modelo: Especifica la ubicación del modelo LLaMA 2 en el sistema de archivos.\n",
        "Tokens Máximos: Define el número máximo de tokens generados en una sola inferencia.\n",
        "Batch Size: Controla el tamaño del lote para la generación de texto.\n",
        "Número de Hilos: Determina la cantidad de hilos utilizados para la computación, afectando el rendimiento.\n",
        "Temperatura: Ajusta la aleatoriedad en la generación de texto, influyendo en la creatividad del output.\n",
        "Ventajas: Alta eficiencia y rendimiento, especialmente en sistemas con limitados recursos.\n",
        "Desventajas: Requiere un ajuste cuidadoso de los parámetros para obtener resultados óptimos.\n",
        "2. GPT4All Groovy\n",
        "\n",
        "Descripción: GPT4All Groovy es una implementación del modelo GPT-4, optimizada para funcionar con el backend gptj, lo que le permite generar texto de alta calidad con una gran variedad de aplicaciones.\n",
        "Características Técnicas:\n",
        "Ruta del modelo: Define la ubicación del modelo GPT4All Groovy.\n",
        "Tokens Máximos: Número máximo de tokens que puede generar en una sesión.\n",
        "Batch Size: Tamaño del lote utilizado durante la generación de texto.\n",
        "Backend: Utiliza gptj como backend, conocido por su balance entre rendimiento y calidad.\n",
        "Temperatura: Controla la aleatoriedad en la generación de texto.\n",
        "Ventajas: Buena calidad en la generación de texto y flexibilidad en su uso.\n",
        "Desventajas: Puede ser más demandante en términos de recursos computacionales.\n",
        "3. GPT4All Snoozy\n",
        "\n",
        "Descripción: GPT4All Snoozy es otra implementación del modelo GPT-4, también optimizada para el backend gptj. Está diseñado para ofrecer un balance entre rendimiento y precisión.\n",
        "Características Técnicas:\n",
        "Ruta del modelo: Ubicación específica del modelo GPT4All Snoozy.\n",
        "Tokens Máximos: Máximo número de tokens generados.\n",
        "Batch Size: Tamaño del lote para la generación de texto.\n",
        "Backend: Utiliza gptj para un rendimiento eficiente.\n",
        "Temperatura: Control de la aleatoriedad en la generación.\n",
        "Ventajas: Ofrece un equilibrio entre precisión y rendimiento.\n",
        "Desventajas: Similar a Groovy, puede ser intensivo en términos de recursos.\n",
        "4. VLLM\n",
        "\n",
        "Descripción: VLLM es un modelo basado en la arquitectura MPT (MosaicML Pre-trained Transformer), optimizado para tareas de generación de texto con gran eficiencia.\n",
        "Características Técnicas:\n",
        "Modelo: Utiliza mosaicml/mpt-7b, un modelo robusto para diversas tareas.\n",
        "Tokens Máximos: Controla el máximo de tokens generados.\n",
        "Top-k: Parámetro que influye en la selección de las próximas palabras, limitando a las k más probables.\n",
        "Top-p: Probabilidad acumulada que define el conjunto de próximas palabras seleccionables.\n",
        "Temperatura: Influencia de la aleatoriedad en la generación de texto.\n",
        "Ventajas: Alta eficiencia y adecuado para tareas que requieren gran cantidad de texto generado.\n",
        "Desventajas: Requiere un ajuste detallado de los parámetros para maximizar el rendimiento.\n",
        "Selección del Mejor Modelo\n",
        "Para seleccionar el mejor modelo, debemos considerar varios factores:\n",
        "\n",
        "Calidad de la Generación de Texto:\n",
        "\n",
        "LLaMA 2: Ideal para sistemas con recursos limitados, pero puede requerir ajustes finos para resultados óptimos.\n",
        "GPT4All Groovy y Snoozy: Ambos ofrecen alta calidad en la generación de texto con flexibilidad en su uso. La elección entre Groovy y Snoozy puede depender de pequeñas diferencias en rendimiento y precisión.\n",
        "VLLM: Excelente para tareas que requieren generación de grandes volúmenes de texto con eficiencia.\n",
        "Rendimiento y Recursos Computacionales:\n",
        "\n",
        "LLaMA 2: Excelente balance entre rendimiento y uso de recursos.\n",
        "GPT4All Groovy y Snoozy: Pueden ser más demandantes en términos de recursos, pero ofrecen alta calidad en generación.\n",
        "VLLM: Alta eficiencia, pero puede necesitar ajustes detallados de parámetros.\n",
        "Flexibilidad y Personalización:\n",
        "\n",
        "LLaMA 2 y VLLM: Ofrecen gran flexibilidad en términos de ajuste de parámetros como n_threads, top_k, y top_p.\n",
        "GPT4All Groovy y Snoozy: Flexibles y ajustables, pero pueden requerir más recursos computacionales."
      ],
      "metadata": {
        "id": "kQoM0go_P-1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store callback handlers (you can add handlers here if needed)\n",
        "callbacks = []\n",
        "\n",
        "# Prepare the Language Model (LLM) based on the specified model_type\n",
        "match model_type:\n",
        "    case \"LLaMA_2_7B\":\n",
        "        # Create an instance of LlamaCpp\n",
        "        llm = LlamaCpp(\n",
        "            model_path = \"/content/drive/MyDrive/Colab/Models/openorca-platypus2-13b.Q4_0.gguf\",  # Path to the LlamaCpp model\n",
        "            max_tokens=model_n_ctx,  # Maximum number of tokens in generated text\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            n_threads=8,           # Number of threads to use\n",
        "            temperature=0.7,       # Sampling temperature\n",
        "            callbacks=callbacks,   # List of callback handlers\n",
        "            verbose=False          # Set to True for verbose output\n",
        "        )\n",
        "    case \"GPT4All_GROOVY\":\n",
        "        # Create an instance of GPT4All\n",
        "        llm = GPT4All(\n",
        "            model=\"/content/drive/MyDrive/Colab/Models/ggml-gpt4all-j-v1.3-groovy.bin\", # Path to the GPT4Al Groovy\n",
        "            max_tokens=model_n_ctx, # Maximum number of tokens in generated text\n",
        "            backend='gpt2',        # Specify the backend (e.g., 'gptj')\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            temperature=0.8,        # Sampling temperature\n",
        "            callbacks=callbacks,    # List of callback handlers\n",
        "            verbose=False           # Set to True for verbose output\n",
        "        )\n",
        "    case \"GPT4All_SNOOZY\":\n",
        "        # Create an instance of GPT4All\n",
        "        llm = GPT4All(\n",
        "            model=\"/content/drive/MyDrive/Colab/Models/GPT4All-13B-snoozy.ggmlv3.q4_0.bin\",# Path to the GPT4All model Snoozy\n",
        "            max_tokens=model_n_ctx, # Maximum number of tokens in generated text\n",
        "            backend='gptj',        # Specify the backend (e.g., 'gptj')\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            temperature=0.75,        # Sampling temperature\n",
        "            callbacks=callbacks,    # List of callback handlers\n",
        "            verbose=False           # Set to True for verbose output\n",
        "        )\n",
        "    case \"VLLM\":\n",
        "        # Create an instance of GPT4All\n",
        "         llm = VLLM(\n",
        "          model=\"mosaicml/mpt-7b\",\n",
        "          trust_remote_code=True,  # mandatory for hf models\n",
        "          max_new_tokens=128,\n",
        "          top_k=10,\n",
        "          top_p=0.95,\n",
        "          temperature=0.8\n",
        "        )\n",
        "\n",
        "    case _default:\n",
        "        # Raise an exception if the model_type is not supported\n",
        "        raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All\")"
      ],
      "metadata": {
        "id": "Ews-tZbd0rgV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicación de la Configuración de Parámetros\n",
        "\n",
        "Ruta del Modelo (model_path / model): Especificar la ruta correcta del modelo es crucial para cargar el archivo adecuado desde el sistema de archivos o desde un repositorio remoto.\n",
        "\n",
        "Tokens Máximos (max_tokens / max_new_tokens): Limitar el número máximo de tokens permite controlar la longitud del texto generado, evitando respuestas excesivamente largas o cortas.\n",
        "\n",
        "Batch Size (n_batch): Ajustar el tamaño del lote puede mejorar la eficiencia del procesamiento y la capacidad del modelo para manejar múltiples solicitudes simultáneamente.\n",
        "\n",
        "Backend: Seleccionar el backend adecuado (gptj en este caso) asegura que el modelo se ejecute de manera óptima en la infraestructura disponible.\n",
        "\n",
        "Callbacks: Los callbacks son útiles para monitorear el progreso y realizar ajustes dinámicos durante la inferencia.\n",
        "\n",
        "Verbose: Activar o desactivar la salida detallada puede ayudar en la depuración y el monitoreo del comportamiento del modelo.\n",
        "\n",
        "Parámetros de Generación (top_k, top_p, temperature): Estos parámetros permiten ajustar la aleatoriedad y la diversidad del texto generado. top_k y top_p limitan la selección de palabras, mientras que temperature ajusta la creatividad del modelo."
      ],
      "metadata": {
        "id": "m2-1MBxcQZj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'hide_source' to True or False as needed\n",
        "hide_source = True  # Set to True to hide source documents, False to show them\n",
        "\n",
        "# Set 'mute_stream' to True or False as needed\n",
        "mute_stream = True  # Set to True to mute stream output, False to allow it\n"
      ],
      "metadata": {
        "id": "TOnUUDaFEOmr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"How many Super Bowls have won Tom Brady?\",\n",
        "    \"How many Territory sold Tula on 202401?\",\n",
        "    \"How many RMAVERICK sold Vehiculos Automotrices De La Piedad on March 2024?\",\n",
        "    \"How many ESCAPE NA FHEV sold Zapata (Suc. Pachuca) on 202401?\",\n",
        "    \"How many TRUCKS sold Vista Hermosa Laredo Motors on 202402?\"\n",
        "]\n",
        "expected_answers = [\n",
        "    \"7\",\n",
        "    \"15\",\n",
        "    \"5\",\n",
        "    \"11\",\n",
        "    \"20\"\n",
        "]\n",
        "\n",
        "answers = []\n",
        "times = []\n",
        "\n",
        "# Process the prompts\n",
        "for i, query in enumerate(prompts):\n",
        "    # Get the answer from the question-answering system\n",
        "    start = time.time()  # Record the start time for performance measurement\n",
        "\n",
        "    print(f\"\\n\\n> Question {i+1}:\")  # Print the query\n",
        "    print(query)\n",
        "    # Create a RetrievalQA instance for question-answering\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever,return_source_documents= False)\n",
        "\n",
        "    res = qa_chain(query)  # Query the question-answering system using the user's query\n",
        "    answer, docs = res['result'], [] if hide_source else res['source_documents']  # Extract answer and source documents\n",
        "    end = time.time()  # Record the end time for performance measurement\n",
        "\n",
        "    # Store the result\n",
        "    answers.append(answer)\n",
        "    times.append(round(end - start, 2))\n",
        "\n",
        "    # Print the result\n",
        "    print(f\"\\n> Expected Answer: {expected_answers[i]}\")  # Print the expected answer\n",
        "    print(f\"\\n> Model Answer (took {times[-1]} s):\")  # Print the model's answer and query response time\n",
        "    print(answer)\n",
        "\n",
        "    # Print the relevant sources used for the answer, if not hiding sources\n",
        "    if not hide_source:\n",
        "        for document in docs:\n",
        "            print(\"\\n> \" + document.metadata[\"source\"] + \":\")  # Print the source document's metadata\n",
        "            print(document.page_content)  # Print the content of the source document\n",
        "\n",
        "# Calculate and display the model's performance\n",
        "average_time = sum(times) / len(times)\n",
        "print(f\"\\n\\nModel Performance:\")\n",
        "print(f\"Average Response Time: {average_time} s\")\n",
        "\n",
        "# Compare model answers with expected answers and calculate accuracy score\n",
        "correct_answers = 0\n",
        "for i, answer in enumerate(answers):\n",
        "    if str(expected_answers[i]).lower() in str(answer).lower():\n",
        "        correct_answers += 1\n",
        "\n",
        "accuracy_score = (correct_answers / len(expected_answers)) * 10\n",
        "print(f\"Accuracy Score: {accuracy_score} / 10\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc8pXBgmIVUH",
        "outputId": "3ff5295a-ec61-4ccc-ea23-0235280fe800"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> Question 1:\n",
            "How many Super Bowls have won Tom Brady?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "> Expected Answer: 7\n",
            "\n",
            "> Model Answer (took 55.39 s):\n",
            " Tom Brady has won 7 Super Bowls.\n",
            "\n",
            "\n",
            "> Question 2:\n",
            "How many Territory sold Tula on 202401?\n",
            "\n",
            "> Expected Answer: 15\n",
            "\n",
            "> Model Answer (took 49.08 s):\n",
            " 15\n",
            "\n",
            "\n",
            "> Question 3:\n",
            "How many RMAVERICK sold Vehiculos Automotrices De La Piedad on March 2024?\n",
            "\n",
            "> Expected Answer: 5\n",
            "\n",
            "> Model Answer (took 84.54 s):\n",
            " The dealer's name is Vehiculos Automotrices De La Piedad. The brand is FORD, and the vehicle line is MAVERICK. The period we are looking for is 202403 (March 2024). However, there isn't any information about the sales of the RMAVERICK in March 2024 provided within these contexts.\n",
            "\n",
            "Final Answer: I don't know.\n",
            "\n",
            "\n",
            "> Question 4:\n",
            "How many ESCAPE NA FHEV sold Zapata (Suc. Pachuca) on 202401?\n",
            "\n",
            "> Expected Answer: 11\n",
            "\n",
            "> Model Answer (took 56.93 s):\n",
            " 11\n",
            "\n",
            "\n",
            "> Question 5:\n",
            "How many TRUCKS sold Vista Hermosa Laredo Motors on 202402?\n",
            "\n",
            "> Expected Answer: 20\n",
            "\n",
            "> Model Answer (took 99.52 s):\n",
            " To find out how many trucks were sold by Vista Hermosa Laredo Motors during the specified time period (202402), we should look at the sales for each vehicle type within that period.\n",
            "\n",
            "For the FORD LOBO CREW, there were 5 sales in 202402.\n",
            "For the FORD F-350, there were 12 sales in 202402.\n",
            "For the FORD TRANSIT V362, there were 13 sales in 202402.\n",
            "\n",
            "There are a total of:\n",
            "14 (LOBO CREW) + 5 (F-350) + 13 (\n",
            "\n",
            "\n",
            "Model Performance:\n",
            "Average Response Time: 69.092 s\n",
            "Accuracy Score: 8.0 / 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultado del Modelo\n",
        "Basándonos en los resultados proporcionados por el modelo LLaMA 2, junto con el contexto adicional de que las respuestas de las preguntas del 2 al 4 provienen de los archivos utilizados para entrenar el modelo, podemos concluir lo siguiente:\n",
        "\n",
        "El modelo LLaMA 2 demostró ser efectivo para proporcionar respuestas precisas a preguntas simples y directas, como la cantidad de Super Bowls ganados por Tom Brady.\n",
        "Las respuestas a las preguntas del 2 al 4, que involucran detalles específicos sobre ventas de vehículos en períodos particulares, fueron generadas a partir de la información contenida en los archivos utilizados para entrenar el modelo. Esto sugiere que el modelo ha aprendido y generalizado efectivamente patrones y detalles relevantes de los datos de entrenamiento.\n",
        "El modelo logró un puntaje de precisión del 80% en la evaluación de las respuestas proporcionadas, lo que indica una capacidad sólida para comprender y responder preguntas con precisión.\n",
        "Dada la precisión general del modelo y su capacidad para proporcionar respuestas relevantes y detalladas, respaldadas por la información contenida en los archivos de entrenamiento, podemos concluir que el modelo LLaMA 2 es una opción sólida y confiable para este escenario particular de pregunta y respuesta."
      ],
      "metadata": {
        "id": "mbbCqa6gRFYW"
      }
    }
  ]
}