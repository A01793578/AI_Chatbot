{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94YPusvub-n",
        "outputId": "5b1cf48f-d0d5-4e12-811d-efb39e4e88a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.2.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.111.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.11.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.18.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.3)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.76)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install gpt4all\n",
        "!pip install chromadb\n",
        "!pip install llama-cpp-python\n",
        "!pip install urllib3\n",
        "!pip install python-dotenv\n",
        "!pip install tqdm\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vR6uFvsv0I0"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Third-Party Library Imports\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Langchain Imports\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader\n",
        ")\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "from transformers import AutoModel, AutoTokenizer  # Only import required submodules from transformers\n",
        "import re\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# ChromaDB Imports\n",
        "from chromadb.config import Settings\n",
        "import chromadb\n",
        "\n",
        "# Argument Parsing\n",
        "import argparse\n",
        "\n",
        "import torch  # Import PyTorch to check GPU availability\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lVGkrCowkZW"
      },
      "outputs": [],
      "source": [
        "persist_directory = \"./db/\"\n",
        "#model_type = \"GPT4All_GPT\"\n",
        "model_type = \"LLaMA_2_7B\"\n",
        "source_directory = \"/content/drive/MyDrive/Colab/SOR/\"\n",
        "model_path = \"/content/drive/MyDrive/Colab/Models/\"\n",
        "embeddings_model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "target_source_chunks = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXhY9iysa-ma",
        "outputId": "ae48dc12-61a0-4c02-ca66-86c845676c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwRoOfK7brJm",
        "outputId": "a0915fc4-be92-4a79-e3c9-87977a4376da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The folder '/content/drive/MyDrive/Colab/SOR/' contains 4 files.\n"
          ]
        }
      ],
      "source": [
        "# List files in the specified source directory\n",
        "archivos = os.listdir(source_directory)\n",
        "\n",
        "# Count the number of files in the directory\n",
        "cantidad_de_archivos = len(archivos)\n",
        "\n",
        "# Print the number of files in the directory\n",
        "print(f\"The folder '{source_directory}' contains {cantidad_de_archivos} files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPbkWegG7X8D",
        "outputId": "44ba8264-b444-4e7f-8ef9-a03675f0efb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GbEaG35wm34",
        "outputId": "01185e41-1ad7-4581-a871-b8911627b326"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create embeddings using Hugging Face model\n",
        "# The 'embeddings_model_name' specifies the pre-trained model to use for embeddings.\n",
        "\n",
        "# Load the model and tokenizer from transformers, specifying the device\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmhR8Aebyeo_"
      },
      "outputs": [],
      "source": [
        "# Map file extensions to document loaders and their arguments\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {})\n",
        "    # Add more mappings for other file extensions and loaders as needed\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otUBD37exX5J"
      },
      "outputs": [],
      "source": [
        "# Function to load a single document based on its file path\n",
        "def load_single_document(file_path: str) -> List[Document]:\n",
        "    # Extract the file extension from the given file path.\n",
        "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
        "\n",
        "    # Check if the file extension is in the LOADER_MAPPING dictionary.\n",
        "    if ext in LOADER_MAPPING:\n",
        "        # Get the loader class and loader arguments for the specified extension.\n",
        "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
        "\n",
        "        # Create an instance of the loader class with the specified file path and arguments.\n",
        "        loader = loader_class(file_path, **loader_args)\n",
        "\n",
        "        # Load the document using the loader and return it.\n",
        "        return loader.load()\n",
        "\n",
        "    # If the file extension is not supported, raise a ValueError.\n",
        "    raise ValueError(f\"Unsupported file extension '{ext}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd4JTeSExbXS"
      },
      "outputs": [],
      "source": [
        "# Function to load documents from the source directory\n",
        "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
        "    # Find all files in the source directory with extensions specified in LOADER_MAPPING.\n",
        "    all_files = []\n",
        "    for ext in LOADER_MAPPING:\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
        "        )\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
        "        )\n",
        "\n",
        "    # Filter out files that are in the ignored_files list.\n",
        "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
        "\n",
        "    # Use a multiprocessing Pool to load documents in parallel.\n",
        "    with Pool(processes=os.cpu_count()) as pool:\n",
        "        results = []\n",
        "        # Create a progress bar for loading documents.\n",
        "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
        "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
        "                results.extend(docs)\n",
        "                pbar.update()\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfG4Hux3xegk"
      },
      "outputs": [],
      "source": [
        "# Function to process documents into text chunks\n",
        "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
        "    # Print a message indicating that documents are being loaded from the specified source directory.\n",
        "    print(f\"Loading documents from {source_directory}\")\n",
        "\n",
        "    # Load documents from the source directory, excluding any ignored files.\n",
        "    documents = load_documents(source_directory, ignored_files)\n",
        "\n",
        "    # Check if there are no documents to process and exit if that's the case.\n",
        "    if not documents:\n",
        "        print(\"No new documents to load\")\n",
        "        exit(0)\n",
        "\n",
        "    # Print the number of loaded documents and the source directory.\n",
        "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
        "\n",
        "    # Create a text splitter with the specified chunk size and overlap.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    # Split the loaded documents into chunks of text using the text splitter.\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Print the number of text chunks created and the maximum chunk size.\n",
        "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
        "\n",
        "    # Return the resulting text chunks.\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIborcoMxhEC"
      },
      "outputs": [],
      "source": [
        "# Function to check if the vector store already exists\n",
        "def does_vectorstore_exist(persist_directory: str, embeddings: HuggingFaceEmbeddings) -> bool:\n",
        "    # Create a Chroma vector store instance with the specified persist directory and embeddings.\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    # Get the 'documents' data from the vector store. If it's empty, return False; otherwise, return True.\n",
        "    if not db.get()['documents']:\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI66G_HcxjSY",
        "outputId": "aa3cf936-8d25-4c0f-b4e6-e43e2c82d788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appending to existing vector store at ./db/\n",
            "Loading documents from /content/drive/MyDrive/Colab/SOR/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Loading new documents: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No new documents to load\n",
            "Loaded 0 new documents from /content/drive/MyDrive/Colab/SOR/\n",
            "Split into 0 chunks of text (max. 500 tokens each)\n",
            "Creating embeddings. May take some minutes...\n",
            "No documents to add. Skipping insertion.\n",
            "Ingestion complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Check if the vector store already exists in the specified directory with the given embeddings.\n",
        "if does_vectorstore_exist(persist_directory, embeddings):\n",
        "    # If the vector store exists, append to it.\n",
        "    print(f\"Appending to existing vector store at {persist_directory}\")\n",
        "\n",
        "    # Create a Chroma vector store instance with the specified directory and embeddings.\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    # Get the existing collection from the vector store.\n",
        "    collection = db.get()\n",
        "\n",
        "    # Extract source file paths from the collection's metadata.\n",
        "    source_file_paths = [metadata['source'] for metadata in collection['metadatas']]\n",
        "\n",
        "    # Process the documents based on the extracted source file paths.\n",
        "    texts = process_documents(source_file_paths)\n",
        "\n",
        "    # Inform the user about the embeddings creation process.\n",
        "    print(f\"Creating embeddings. May take some minutes...\")\n",
        "\n",
        "    # Check if 'texts' is not empty before adding documents to the vector store\n",
        "    if texts:\n",
        "        # Add the processed documents to the vector store.\n",
        "        db.add_documents(texts)\n",
        "    else:\n",
        "        print(\"No documents to add. Skipping insertion.\")\n",
        "else:\n",
        "    # If the vector store does not exist, create a new one.\n",
        "    print(\"Creating a new vector store\")\n",
        "\n",
        "    # Process documents without specifying ignored files (default behavior).\n",
        "    texts = process_documents()\n",
        "\n",
        "    # Inform the user about the embeddings creation process.\n",
        "    print(f\"Creating embeddings. May take some minutes...\")\n",
        "\n",
        "    # Create a new Chroma vector store with the processed documents and embeddings.\n",
        "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "# Persist the vector store.\n",
        "db.persist()\n",
        "\n",
        "# Clear the db variable to free up resources.\n",
        "db = None\n",
        "\n",
        "# Inform the user that the ingestion process is complete.\n",
        "print(f\"Ingestion complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk7vv2QczkIx",
        "outputId": "a2f8ddad-b0c3-4e61-b529-f422398c36e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An instance of Chroma already exists for ./db/ with different settings. Reusing the existing instance.\n"
          ]
        }
      ],
      "source": [
        "# Create settings for Chroma database configuration\n",
        "settings = Settings(\n",
        "    persist_directory=persist_directory,  # Directory for persisting database data\n",
        "    anonymized_telemetry=False  # Disable anonymized telemetry\n",
        ")\n",
        "\n",
        "# Create Hugging Face embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "# Try to create a persistent Chroma database client\n",
        "try:\n",
        "    chroma_client = chromadb.PersistentClient(\n",
        "        settings=settings,  # Database settings\n",
        "        path=persist_directory  # Path to the database directory\n",
        "    )\n",
        "except ValueError as e:\n",
        "    # Handle the case where an instance already exists with different settings\n",
        "    print(f\"An instance of Chroma already exists for {persist_directory} with different settings. Reusing the existing instance.\")\n",
        "    chroma_client = chromadb.PersistentClient(\n",
        "        path=persist_directory  # Path to the database directory\n",
        "    )\n",
        "\n",
        "# Create a Chroma vector store instance\n",
        "db = Chroma(\n",
        "    persist_directory=persist_directory,  # Directory for persisting vector store data\n",
        "    embedding_function=embeddings,  # Embeddings function\n",
        "    client_settings=settings,  # Database settings\n",
        "    client=chroma_client  # Chroma client\n",
        ")\n",
        "\n",
        "# Create a retriever for document retrieval\n",
        "retriever = db.as_retriever(\n",
        "    search_kwargs={\"k\": target_source_chunks}  # Search settings (e.g., number of search results)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ews-tZbd0rgV"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store callback handlers (you can add handlers here if needed)\n",
        "callbacks = []\n",
        "model_n_batch = 64\n",
        "model_n_ctx = 1024\n",
        "\n",
        "# Prepare the Language Model (LLM) based on the specified model_type\n",
        "match model_type:\n",
        "    case \"LLaMA_2_7B\":\n",
        "        # Create an instance of LlamaCpp\n",
        "        llm = LlamaCpp(\n",
        "            model_path =  model_path +\"openorca-platypus2-13b.Q4_0.gguf\",  # Path to the LlamaCpp model\n",
        "            max_tokens=model_n_ctx,  # Maximum number of tokens in generated text\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            n_threads=8,           # Number of threads to use\n",
        "            temperature=.8,       # Sampling temperature\n",
        "            callbacks=callbacks,   # List of callback handlers\n",
        "            verbose=False          # Set to True for verbose output\n",
        "        )\n",
        "    case \"GPT4All_GPT2\":\n",
        "        # Create an instance of GPT4All\n",
        "        llm = GPT4All(\n",
        "            model = model_path + \"GPT4All-13B-snoozy.ggmlv3.q4_0.bin\",# Path to the GPT4All model Snoozy\n",
        "            max_tokens=model_n_ctx, # Maximum number of tokens in generated text\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            callbacks=callbacks,    # List of callback handlers\n",
        "            verbose=False           # Set to True for verbose output\n",
        "        )\n",
        "    case \"GPT4All_GPT\":\n",
        "        # Create an instance of GPT4All\n",
        "        llm = GPT4All(\n",
        "            model = model_path + \"ggml-gpt4all-j-v1.3-groovy.bin\",# Path to the GPT4All model Snoozy\n",
        "            max_tokens=model_n_ctx, # Maximum number of tokens in generated text\n",
        "            backend='gptj',        # Specify the backend (e.g., 'gptj')\n",
        "            n_batch=model_n_batch,  # Batch size for text generation\n",
        "            callbacks=callbacks,    # List of callback handlers\n",
        "            verbose=False           # Set to True for verbose output\n",
        "        )\n",
        "    case _default:\n",
        "        # Raise an exception if the model_type is not supported\n",
        "        raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOnUUDaFEOmr"
      },
      "outputs": [],
      "source": [
        "# Set 'hide_source' to True or False as needed\n",
        "hide_source = True  # Set to True to hide source documents, False to show them\n",
        "\n",
        "# Set 'mute_stream' to True or False as needed\n",
        "mute_stream = True  # Set to True to mute stream output, False to allow it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5tD8nmA7gJj"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"How many ESCAPE NA FHEV sold the dealer Tula on 202403?\",\n",
        "    \"How many MUSTANG sold the dealer Zapata on 202402?\"\n",
        "]\n",
        "expected_answers = [\n",
        "    \"4\",\n",
        "    \"3\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwhP419gt8iU"
      },
      "outputs": [],
      "source": [
        "# Define evaluation prompts\n",
        "context_relevance_prompt = \"\"\"\n",
        "You are a RELEVANCE grader; providing the relevance of the given STATEMENT to the given QUESTION.\n",
        "Answer only a number from number 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n",
        "QUESTION: {query}\n",
        "STATEMENT: {context}\n",
        "\"\"\"\n",
        "\n",
        "groundedness_prompt = \"\"\"\n",
        "You are a GROUNDEDNESS grader; providing the groundedness of the given RESPONSE to the given CONTEXT.\n",
        "Answer only a number from number 0 to 10 where 0 is the least grounded and 10 is the most relevant.\n",
        "CONTEXT: {context}\n",
        "RESPONSE: {response}\n",
        "\"\"\"\n",
        "\n",
        "answer_faithfulness_prompt = \"\"\"\n",
        "You are an faithfulness grader; providing the relevance of the given ANSWER to the given QUESTION.\n",
        "Answer only a number from number 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n",
        "QUESTION: {query}\n",
        "ANSWER: {response}\n",
        "\"\"\"\n",
        "\n",
        "# Function to evaluate context relevance\n",
        "def evaluate_context_relevance(query: str, context: str) -> int:\n",
        "    prompt = context_relevance_prompt.format(query=query, context=context)\n",
        "    response = llm(prompt)\n",
        "    try:\n",
        "        relevance_match = re.search(r\"Relevance:\\s*(\\d+\\.?\\d*)\", response)\n",
        "        if relevance_match:\n",
        "            return int(float(relevance_match.group(1)))\n",
        "        else:\n",
        "            print(f\"Relevance score not found in response: {response}\")\n",
        "            return 0\n",
        "    except ValueError:\n",
        "        print(f\"Invalid response for context relevance: {response}\")\n",
        "        return 0  # Default to 0 if the response is invalid\n",
        "\n",
        "# Function to evaluate groundedness\n",
        "def evaluate_groundedness(context: str, response: str) -> int:\n",
        "    prompt = groundedness_prompt.format(context=context, response=response)\n",
        "    response = llm(prompt)\n",
        "    try:\n",
        "        groundedness_match = re.search(r\"GROUNDEDNESS:\\s*(\\d+\\.?\\d*)\", response)\n",
        "        if groundedness_match:\n",
        "            return int(float(groundedness_match.group(1)))\n",
        "        else:\n",
        "            print(f\"Groundedness score not found in response: {response}\")\n",
        "            return 0\n",
        "    except ValueError:\n",
        "        print(f\"Invalid response for groundedness: {response}\")\n",
        "        return 0  # Default to 0 if the response is invalid\n",
        "\n",
        "# Function to evaluate answer relevance\n",
        "def evaluate_answer_relevance(query: str, response: str) -> int:\n",
        "    prompt = answer_faithfulness_prompt.format(query=query, response=response)\n",
        "    response = llm(prompt)\n",
        "    try:\n",
        "        relevance_match = re.search(r\"Relevance score:\\s*(\\d+\\.?\\d*)\", response)\n",
        "        if relevance_match:\n",
        "            return int(float(relevance_match.group(1)))\n",
        "        else:\n",
        "            print(f\"Relevance score not found in response: {response}\")\n",
        "            return 0\n",
        "    except ValueError:\n",
        "        print(f\"Invalid response for answer relevance: {response}\")\n",
        "        return 0  # Default to 0 if the response is invalid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5sUKTPycSU9",
        "outputId": "b6cc2844-d7b0-4d04-cba1-26a0785de46a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> Prompt: How many ESCAPE NA FHEV sold the dealer Tula on 202403?\n",
            "> Expected Answer: 4\n",
            "> Model Answer: The dealer Tula sold 4 ESCAPE NA FHEV vehicles on 202403.\n",
            "\n",
            "> Context: Dealer Name: Tula\n",
            "Brand: FORD\n",
            "Period: 202403\n",
            "Time Period: MTD\n",
            "Vehicle Line: ESCAPE NA FHEV\n",
            "Vehicle Type: OUTFITTERS\n",
            "Vehicles Sales Total: 4 Dealer Name: Tula\n",
            "Brand: FORD\n",
            "Period: 202404\n",
            "Time Period: MTD\n",
            "Vehicle Line: TRANSIT NA\n",
            "Vehicle Type: TRUCKS\n",
            "Vehicles Sales Total: 5 Dealer Name: Tula\n",
            "Brand: FORD\n",
            "Period: 202404\n",
            "Time Period: MTD\n",
            "Vehicle Line: F-350\n",
            "Vehicle Type: TRUCKS\n",
            "Vehicles Sales Total: 8 Dealer Name: Tula\n",
            "Brand: FORD\n",
            "Period: 202403\n",
            "Time Period: MTD\n",
            "Vehicle Line: MAVERICK\n",
            "Vehicle Type: TRUCKS\n",
            "Vehicles Sales Total: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relevance score not found in response: \n",
            "In this statement, there is no mention of the sales number for ESCAPE NA FHEV at dealer Tula during 202403. The provided information gives data for different vehicle lines and time periods but not specifically for the requested question. Therefore, the relevance score is 0.\n",
            "\n",
            "RELEVANCE: 0\n",
            "Relevance score not found in response: \n",
            "Answer: 7 (The given information indicates that the question is about a specific number of sales, but it is not provided directly in the text. It can be inferred from \"Tula sold 4 ESCAPE NA FHEV\" which implies this happened on one occasion - 202403.)\n",
            "> Context Relevance: 0\n",
            "> Groundedness: 7\n",
            "> Answer Relevance: 0\n",
            "\n",
            "> Prompt: How many MUSTANG sold the dealer Zapata on 202402?\n",
            "> Expected Answer: 3\n",
            "> Model Answer: 3\n",
            "\n",
            "> Context: Dealer Name: Zapata\n",
            "Brand: FORD\n",
            "Period: 202401\n",
            "Time Period: MTD\n",
            "Vehicle Line: MUSTANG\n",
            "Vehicle Type: CARS\n",
            "Vehicles Sales Total: 14 Dealer Name: Zapata\n",
            "Brand: FORD\n",
            "Period: 202404\n",
            "Time Period: MTD\n",
            "Vehicle Line: MUSTANG\n",
            "Vehicle Type: CARS\n",
            "Vehicles Sales Total: 7 Dealer Name: Zapata\n",
            "Brand: FORD\n",
            "Period: 202402\n",
            "Time Period: MTD\n",
            "Vehicle Line: MUSTANG\n",
            "Vehicle Type: CARS\n",
            "Vehicles Sales Total: 3 Dealer Name: Zapata (Suc. Aeropuerto)\n",
            "Brand: FORD\n",
            "Period: 202401\n",
            "Time Period: MTD\n",
            "Vehicle Line: MUSTANG\n",
            "Vehicle Type: CARS\n",
            "Vehicles Sales Total: 12\n",
            "Relevance score not found in response: \n",
            "There are no records of Mustang sales for dealer Zapata on 202402. The most relevant information available is from the period 202401, which had a total of 12 MUSTANG sold by the dealer Zapata. \n",
            "So, the relevance score is 12.\n",
            "Relevance score not found in response: \n",
            "I cannot provide a relevance score for this question, as it appears to be unrelated or lacking proper context. The provided answer does not seem to correlate with any information about car sales or dealerships.\n",
            "\n",
            "Please provide additional context or clarification regarding the question's subject matter so that I can evaluate and assign a relevant response.<|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|><|end_of_turn|>\n",
            "> Context Relevance: 0\n",
            "> Groundedness: 3\n",
            "> Answer Relevance: 0\n",
            "\n",
            "\n",
            "Model Performance:\n",
            "Average Response Time: 52.065 s\n",
            "Average Context Relevance: 0.0 / 10\n",
            "Average Groundedness: 5.0 / 10\n",
            "Average Answer Relevance: 0.0 / 10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "answers = []\n",
        "times = []\n",
        "context_relevance_scores = []\n",
        "groundedness_scores = []\n",
        "answer_relevance_scores = []\n",
        "\n",
        "# Process the prompts\n",
        "for i, query in enumerate(prompts):\n",
        "    start = time.time()\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
        "    res = qa_chain(query)\n",
        "    answer, docs = res['result'], res['source_documents']\n",
        "    end = time.time()\n",
        "\n",
        "    # Use regex to extract the first sentence from the result\n",
        "    first_sentence = re.split(r'\\.\\s', answer, 1)[0].strip()\n",
        "\n",
        "    print(f\"\\n> Prompt: {query}\")\n",
        "    print(f\"> Expected Answer: {expected_answers[i]}\")\n",
        "    print(f\"> Model Answer: {first_sentence}\") # Print the model's answer and query response time\n",
        "\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "    print(f\"\\n> Context: {context}\")\n",
        "    context_relevance = evaluate_context_relevance(query, context)\n",
        "    groundedness = evaluate_groundedness(context, answer)\n",
        "    answer_relevance = evaluate_answer_relevance(query, answer)\n",
        "\n",
        "    context_relevance_scores.append(context_relevance)\n",
        "    groundedness_scores.append(groundedness)\n",
        "    answer_relevance_scores.append(answer_relevance)\n",
        "    answers.append(answer)\n",
        "    times.append(round(end - start, 2))\n",
        "\n",
        "    # Output the result\n",
        "    print(f\"> Context Relevance: {context_relevance}\")\n",
        "    print(f\"> Groundedness: {groundedness}\")\n",
        "    print(f\"> Answer Relevance: {answer_relevance}\")\n",
        "\n",
        "# Calculate and display the model's performance\n",
        "average_time = sum(times) / len(times)\n",
        "average_context_relevance = sum(context_relevance_scores) / len(context_relevance_scores)\n",
        "average_groundedness = sum(groundedness_scores) / len(groundedness_scores)\n",
        "average_answer_relevance = sum(answer_relevance_scores) / len(answer_relevance_scores)\n",
        "\n",
        "print(f\"\\n\\nModel Performance:\")\n",
        "print(f\"Average Response Time: {average_time} s\")\n",
        "print(f\"Average Context Relevance: {average_context_relevance} / 10\")\n",
        "print(f\"Average Groundedness: {average_groundedness} / 10\")\n",
        "print(f\"Average Answer Relevance: {average_answer_relevance} / 10\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}